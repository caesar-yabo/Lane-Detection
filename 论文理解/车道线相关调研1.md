# 一、Towards End-to-End Lane Detection: an Instance Segmentation Approach（2018.2）

## 1.数据集

- tusimple
- 数据集介绍：
  - 主要是高速路上车道数据；
  - 训练集中有3626个flips，每个flips包括20帧图片，最后一帧有标注信息；
  - 原始图片大小：1280X720；
  - 车道线标注格式（json格式）；
  - ![](.\images\tusimpleData.PNG)
  - h_sample:将图像在高度上等距离划分若干点；
  - lanes:列表个数表示该图片中包含车道线的个数，对于每一个列表，记录的是该车道线所处位置的宽度位置；
  - 标注的信息无法区分双实线、白线、黄线等具体类型的车道线；所有车道线统一对待；

## 2.适用类型和环境场景

- 类型
  - 直道（可以）
  - 弯道（可以）
  - 上坡、下坡（可以）
  - 低速、高速（得看模型最后的运行速度）
  - 实线/虚线/实现+虚线/双实线（不可以）
- 环境场景
  - 白天（可以）
  - 晚上（待查）
  - 雨天（待查）
  - 强光（待查）
  - 雪天（待查）
  - 雾天（待查）



## 3.算法

![](.\images\Lanet1.png)



- 将车道检测问题转化为实例分割问题，每个车道线形成独立的实例；
- 可处理不固定车道线和车道变化；
- 在拟合车道线时，提出H-Net，用于学习透视变换矩阵，较之前固定且预先定义的变换矩阵相比，提高了道路平面变化情况下的车道线拟合的鲁棒性。
- 模型输入图片大小：512X256

![](.\images\Lanet.png)



### binary segmentation branch

- 用于训练输出得到一个二值化的分割图，白色代表车道线，黑色代表背景；
- loss：交叉熵损失函数；
- 类别不平衡： apply bounded inverse class weighting；
- 构建ground-truth分割图时，将每个车道线的对应像素连成线；好处是车道线被遮挡了或车道线消失，网络仍能预测车道位置。



### instance embeding branch

- 目的：区分binary segmentation得到车道线，每条车道具体包含哪些像素点;

- **pixel embedding**: mapping each pixel to a point in n-d feature space

- 具体做法参考了论文：Semantic Instance Segmentation for Autonomous Driving和Semantic Instance Segmentation with a Discriminative Loss Function，提出的针对实例分割提出的LossFunction

  - 实例分割任务中，softmax loss的缺陷：

    - 应用 `softmax loss` 的网络的最后输出层的 `channel` 数等于类别数，因为图像中的实例数目不定，所以网络最后层的结构无法确定。

    - 如果网络每个像素的预测输出和 `ground truth`的`id-label`不一致 ，如下图：

    - ![](D:/myLaneDetection/papers/%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/images/image3.png)

      `softmax loss` 会惩罚这种 **预测错误**。 但是: 这种预测结果其实是对的，只要不同实例之间的 `id-label` 不同就可以，即 `instance id label` 满足 `permutation-invariant`( “**置换不变性**”)的性质。

  - ![](D:/myLaneDetection/papers/%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/images/Loss.PNG)

  - 用新提出的 `loss function` 对网络进行训练。优化目标是： 网络将图像每个像素投影到 `n-d`特征空间（`n` 是个随数据集变化而变化的超参），使得同属于一个实例的像素尽量靠近，形成一个 `cluster`, 每一个实例对应一个 `cluster`, 不同 `cluster`则尽量远离。

  - ![](D:/myLaneDetection/papers/%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/images/Loss1.PNG)

  - 终止聚类的条件是：车道聚类（即各车道线间间距）中心间距离>δd,每个类（每条车道线）中包含的车道线像素离该车道线距离<δv。

    两个分支的loss权重相同。

### 最后聚类过程：

- 先利用binary segmentation branch得到的结果对instance embeding branch得到的embeding做掩码，然后再聚类。

- 设置 δd > 6δv为迭代终止条件，使上述的loss做迭代。（6为超参数）

  ​	

### CURVE FITTING USING H-NET

![](D:/myLaneDetection/papers/%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/images/space_chansform.PNG)

流行的拟合模型有三次多项式，样条曲线，回旋曲线。为了提高拟合质量且保持计算效率，通常将图像转到鸟瞰图后做拟合。最后再逆变换到原图即可。但有个问题是：这个透视变换矩阵会受地面线影响（如上坡）。本文在做曲线拟合前先训练一个网络用于生成透视变换矩阵系数以解决道路平面变动(上下坡)的影响。

![](D:/myLaneDetection/papers/%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/images/H-Net.PNG)



转换矩阵6个自由度：（另外三个固定，是为了保证水平线在转换后的空间仍保持水平，增加了限制条件）

![](./images/H_Net_transform.PNG)



### 评价标准

在tusimple数据集上以准确率为评价标准。(预测准确的点/ground truth中的点)

准确率的计算：

![](.\images\ACC.PNG)







## 4.结果

- ![](./images/RANK.PNG)
- 在tusimple数据集上准确率为96.4%。
- 训练模型的硬件：a NVIDIA 1080Ti
- 速度：52fps



## 5.优点与改进

- 可以处理车道变化的情况，车道数量可以有变化；

- 在拟合车道线时，提出H-Net，用于学习透视变换矩阵，较之前固定且预先定义的变换矩阵相比，提高了道路平面变化情况下的车道线拟合的鲁棒性。

  



# 二、Spatial As Deep: Spatial CNN for Traffic Scene Understanding

## 1.数据集（待找）

- 自己收集的数据集：

  - 以前的车道检测数据集(KITTI,CamVid)要不就是太简单，要不就是数据太小。最近的(Caltech,TuSimple)数据集是在交通受限状态下建立的，这样的数据车流量少且路标较为清晰。这些数据集没有包括一些车道线模糊，条件恶劣的情况，而这些情况人类可以推断出来，且这具有很高的实用价值。

  - 论文提出的数据集是由六辆车在北京不同时间录制的，超过55个小时共收集了133,235 张图片，这超过TuSimple 数据集20倍了。论文分成88880张作为训练集, 9675作为验证集，34680做测试集。图像的大小为1640×590

  - ![](.\images\BeiJingData.PNG)

  - 数据集内包括城市、农村和高速公路等场景，北京作为世界上最大和最拥挤的城市之一，对应的车道检测数据提供了很多具有挑战性的交通场景。论文将测试集分为正常和8个具有挑战性的类别，这对应上图 (a)的9个示例情况。图(b)显示的是挑战性的场景站数据集的比例(共72.3%)。

    对于每一张图片，使用三条线注释车道，如前面所述，许多情况下车道是被遮挡的或看不见的。而这在实际情况下是很重要的，车道检测算法需要能够在这种情况下工作。对此，标注工作根据上下文也做了标注，如图(2)(4)所示。对于图(1)的情况我们不对障碍的另一边做标注，将精力集中于最受关注的部分。





## 2.适用类型和环境场景

最多检测三车道，四个车道线

- 类型
  - 直道（可以）
  - 弯道（可以）
  - 上坡、下坡（可以）
  - 低速、高速（得看模型最后的运行速度）
  - 实线/虚线/实现+虚线/双实线（不可以）
- 环境场景
  - 白天（可以）
  - 晚上（可以）
  - 强光（可以）
  - 拥挤（可以）
  - 无车道线（可以）
  - 阴影（可以）
  - 十字路口（可以）
  - 雨天（待查）
  - 雪天（待查）
  - 雾天（待查）

## 3.算法

### Spatial CNN

- 传统关于空间关系的建模方法

  - 基于概率图模型：马尔科夫随机场(MRF)或条件随机场(CRF)
  - 概率图与CNN结合
    - ![](.\images\gailvtu_CNN.PNG)
    - 具体过程分为：
      - 标准化：CNN的输出作为一元势函数，并通过Softmax操作标准化
      - 信息传递：可通过大内核的逐通道卷积实现(对于DenseCRF,内核大小将覆盖整张图片，内核权重取决于图片)
      - 兼容性转换：使用1×1的卷积实现
      - 添加一元势：整个过程迭代N次得到最终输出
    - 缺点：传统方法在传递信息时，每个像素点接受来自全图其他像素的信息，这在计算上是非常昂贵的，难以应用于实时系统。且对于MRF的大卷积核权重很难学。这些方法是应用在CNN的输出上的，论文认为CNN的隐藏层，包含了丰富的空间关系，可更好的用于处理空间关系。
    - 

- SCNN

  - Instead of viewing different lane markings as one class and do clustering
    afterwards, we want the neural network to distinguish different lane markings on itself, which could be more robust.
    Thus these four lanes are viewed as different classes. Moreover, the probmaps are then sent to a small network to give
    prediction on the existence of lane markings.

  - ![](.\images\SCNN.PNG)

  - 图中SCNN的下标有`D,U,R,L`，这在结构上是类似的，方向上分别表示为向下，向上，向右，向左

  - 计算过程：

    - 先以`SCNN_D`分析：
    - ![](.\images\SCNN_compute.PNG)

  - Loss和输出是什么？

  - ![](.\images\SCNN_structure.PNG)

    



## 4.结果

![](./images/RANK.PNG)

- 在tusimple数据集上准确率为96.53%。
- 训练模型的硬件：GeForce GTX TITAN Black
- 速度：42ms







## 5.优点与改进

- SCNN相比于传统方法的优势：
  - 计算效率
    - ![](.\images\12.PNG)
    - 图(a)：MRF/CRF中每个像素点会直接接收其他所有像素点的信息(大卷积核实现)，这其中有许多冗余计算。图(b)：在SCNN中，信息是顺序传递的。
- 将传递信息作残差
  - 密集的MRF/CRF内是通过所有加权像素相加，这样的计算花费很大。而RNN是通过梯度来优化的， 考虑到这么多层和列，依据残差网络的经验，论文也采用残差的方式来学习(计算公式描述的残差学习)。这种残差可认为是对原始神经元的修正。实验证明这样的消息传递比基于LSTM的要好。
  - 灵活性
    - 归功于SCNN的计算效率，它可以很方便的集成到CNN的任何部分。通常 top hidden layer 包含了丰富的语义信息，这是应用SCNN的理想位置。在完整的SCNN模型中我们在顶层的 feature map上用了四个方向的SCNN引入空间信息传递。





# 三、Robust Lane Detection from Continuous Driving Scenes Using Deep Neural Networks

## 1.数据集

- tusimple；

## 2.适用类型和环境场景

尤其是在复杂场景下的车道线检测上表现突出

## 3.算法

- encoder-decoder编解码框架
  - ![](.\images\encoder_decoder.png)
  - 编码CNN和解码CNN是两个全连接网络。一系列的连续图片作为输入，传入编码CNN模块，编码CNN处理后得到时序的特征图。特征图作为输入传入LSTM网络用来预测车道线信息。LSTM网络的输出再传入到解码CNN网络中，得到车道线预测的概率图。且这个概率图和输入图片尺寸大小一致。
  - 两个Encoder框架：(采用SegNe和UNet在ImageNet上的预训练权重模型)
    - ![](.\images\two_encoders6.png)
- LSTM network
  - LSTM细胞在t时刻的激活函数可以表示为如下：
  - ![](.\images\LSTM.PNG)
- 损失函数
  - ![](.\images\loss_1.PNG)
  - ![](.\images\loss_2.PNG)

## 4.结果

- 输入图片大小：256X128;
- 连续图片个数：5
- 硬件： 64GB RAM and two GeForce GTX TITAN-X GPU（12G X 2）
- ![](.\images\result1.PNG)
- ![](.\images\result2.PNG)
- 时间消耗：
  - 由于加入了LSTM模块，运行速度下降；
  - 在线运行时，前四张图片的信息已经抽取，只需处理当前图片，速度与只输入一张图片的模型相当。（ the encoder network only need to process the current frame since the previous frames have already been abstracted）

## 5.优点与改进

- 利用连续多个帧进行车道线检测，提出了结合CNN+RNN的混合深度框架；
- 在复杂场景中如重阴影，标记线严重退化，堵车等场景中表现突出；

# 四、A Novel Vision-Based Framework for Real-Time Lane Detection and Tracking（2019）

- 提出了一个新的框架，但论文中没有关于速度和准确率的具体信息。
- 通过Kalman filter来追踪车道线，可以提升车道检测稳定性（The Kalman filter can improve lane detection performance, especially in the conditions of various illumination, worn lane lines or strong disturbance.Kalman filter is chosen to track the lanes so that when the lane line disappears suddenly, current framecan be processed by taking detection result of previous frameinto consideration.）
- 数据集选用CULanes DataSet;
- ![](.\images\newStructure.PNG)
- ![](.\images\FlowPipeLine.PNG)
- 

# 五、LineNet: a Zoomable CNN for Crowdsourced High Definition Maps Modeling in Urban Environments（2018）

- 可处理双黄线、实线、虚线等具体类型；
- six branches in our LP layer: line mask; line type; line position; line direction; line confidence; and line distance
  - **line mask**：is a stroke we draw with a fixed width (32 pixels in our experiments).
  - **line type**：indicates one of the six-lane marking types（WS(WHITE SOLID), WD(WHITE DASH), RB(ROAD BOUNDARIES), YS(YELLOW SOLID), YD(YELLOW DASH)，others）
  - **line position**：Line position predicts the vector from an anchor point to the closest point in the line. Supervising on the line position can produce much more accurate results than using a mask.线位置预测从anchor 到线中最近点的矢量。 对线位置进行监控可以比使用mask产生更准确的结果。
  - **Line direction**：predicts the orientation of the lane
  - **Line confidence**：predicts the confidence ratio, i.e., whether the network can see the lane clear enough, which is defined between 0 (if two lines are closer than 46 pixels) and 1 (otherwise).When we zoom into two adjacent lines, they will gradually separates, result in change of the confidence ratio.
  - **Line distance**：the distance from the anchor point to the closest point in the line
