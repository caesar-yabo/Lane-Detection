{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import os.path as ops\n",
    "import random\n",
    "\n",
    "import glog as log\n",
    "import tensorflow as tf\n",
    "\n",
    "import global_config\n",
    "import tf_io_pipline_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = global_config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_args():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset_dir', type=str, help='The source nsfw data dir path')\n",
    "    parser.add_argument('--tfrecords_dir', type=str, help='The dir path to save converted tfrecords')\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneNetDataProducer(object):\n",
    "    \"\"\"\n",
    "    Convert raw image file into tfrecords\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_dir):\n",
    "        \"\"\"\n",
    "        :param dataset_dir:\n",
    "        \"\"\"\n",
    "        self._dataset_dir = dataset_dir\n",
    "\n",
    "        self._gt_image_dir = ops.join(dataset_dir, 'gt_image')\n",
    "        self._gt_binary_image_dir = ops.join(dataset_dir, 'gt_binary_image')\n",
    "        self._gt_instance_image_dir = ops.join(dataset_dir, 'gt_instance_image')\n",
    "\n",
    "        self._train_example_index_file_path = ops.join(self._dataset_dir, 'train.txt')\n",
    "        self._test_example_index_file_path = ops.join(self._dataset_dir, 'test.txt')\n",
    "        self._val_example_index_file_path = ops.join(self._dataset_dir, 'val.txt')\n",
    "\n",
    "        if not self._is_source_data_complete():\n",
    "            raise ValueError('Source image data is not complete, '\n",
    "                             'please check if one of the image folder is not exist')\n",
    "\n",
    "        if not self._is_training_sample_index_file_complete():\n",
    "            self._generate_training_example_index_file()\n",
    "\n",
    "    def generate_tfrecords(self, save_dir, step_size=10000):\n",
    "        \"\"\"\n",
    "        Generate tensorflow records file\n",
    "        :param save_dir:\n",
    "        :param step_size: generate a tfrecord every step_size examples\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        def _read_training_example_index_file(_index_file_path):\n",
    "\n",
    "            assert ops.exists(_index_file_path)\n",
    "\n",
    "            _example_gt_path_info = []\n",
    "            _example_gt_binary_path_info = []\n",
    "            _example_gt_instance_path_info = []\n",
    "\n",
    "            with open(_index_file_path, 'r') as _file:\n",
    "                for _line in _file:\n",
    "                    _example_info = _line.rstrip('\\r').rstrip('\\n').split(' ')\n",
    "                    _example_gt_path_info.append(_example_info[0])\n",
    "                    _example_gt_binary_path_info.append(_example_info[1])\n",
    "                    _example_gt_instance_path_info.append(_example_info[2])\n",
    "\n",
    "            ret = {\n",
    "                'gt_path_info': _example_gt_path_info,\n",
    "                'gt_binary_path_info': _example_gt_binary_path_info,\n",
    "                'gt_instance_path_info': _example_gt_instance_path_info\n",
    "            }\n",
    "\n",
    "            return ret\n",
    "\n",
    "        def _split_writing_tfrecords_task(\n",
    "                _example_gt_paths, _example_gt_binary_paths, _example_gt_instance_paths, _flags='train'):\n",
    "\n",
    "            _split_example_gt_paths = []\n",
    "            _split_example_gt_binary_paths = []\n",
    "            _split_example_gt_instance_paths = []\n",
    "            _split_tfrecords_save_paths = []\n",
    "\n",
    "            for i in range(0, len(_example_gt_paths), step_size):\n",
    "                _split_example_gt_paths.append(_example_gt_paths[i:i + step_size])\n",
    "                _split_example_gt_binary_paths.append(_example_gt_binary_paths[i:i + step_size])\n",
    "                _split_example_gt_instance_paths.append(_example_gt_instance_paths[i:i + step_size])\n",
    "\n",
    "                if i + step_size > len(_example_gt_paths):\n",
    "                    _split_tfrecords_save_paths.append(\n",
    "                        ops.join(save_dir, '{:s}_{:d}_{:d}.tfrecords'.format(_flags, i, len(_example_gt_paths))))\n",
    "                else:\n",
    "                    _split_tfrecords_save_paths.append(\n",
    "                        ops.join(save_dir, '{:s}_{:d}_{:d}.tfrecords'.format(_flags, i, i + step_size)))\n",
    "\n",
    "            ret = {\n",
    "                'gt_paths': _split_example_gt_paths,\n",
    "                'gt_binary_paths': _split_example_gt_binary_paths,\n",
    "                'gt_instance_paths': _split_example_gt_instance_paths,\n",
    "                'tfrecords_paths': _split_tfrecords_save_paths\n",
    "            }\n",
    "\n",
    "            return ret\n",
    "\n",
    "        # make save dirs\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # start generating training example tfrecords\n",
    "        log.info('Start generating training example tfrecords')\n",
    "\n",
    "        # collecting train images paths info\n",
    "        train_image_paths_info = _read_training_example_index_file(self._train_example_index_file_path)\n",
    "        train_gt_images_paths = train_image_paths_info['gt_path_info']\n",
    "        train_gt_binary_images_paths = train_image_paths_info['gt_binary_path_info']\n",
    "        train_gt_instance_images_paths = train_image_paths_info['gt_instance_path_info']\n",
    "\n",
    "        # split training images according step size\n",
    "        train_split_result = _split_writing_tfrecords_task(\n",
    "            train_gt_images_paths, train_gt_binary_images_paths, train_gt_instance_images_paths, _flags='train')\n",
    "        train_example_gt_paths = train_split_result['gt_paths']\n",
    "        train_example_gt_binary_paths = train_split_result['gt_binary_paths']\n",
    "        train_example_gt_instance_paths = train_split_result['gt_instance_paths']\n",
    "        train_example_tfrecords_paths = train_split_result['tfrecords_paths']\n",
    "\n",
    "        for index, example_gt_paths in enumerate(train_example_gt_paths):\n",
    "            tf_io_pipline_tools.write_example_tfrecords(\n",
    "                example_gt_paths,\n",
    "                train_example_gt_binary_paths[index],\n",
    "                train_example_gt_instance_paths[index],\n",
    "                train_example_tfrecords_paths[index]\n",
    "            )\n",
    "\n",
    "        log.info('Generating training example tfrecords complete')\n",
    "\n",
    "        # start generating validation example tfrecords\n",
    "        log.info('Start generating validation example tfrecords')\n",
    "\n",
    "        # collecting validation images paths info\n",
    "        val_image_paths_info = _read_training_example_index_file(self._val_example_index_file_path)\n",
    "        val_gt_images_paths = val_image_paths_info['gt_path_info']\n",
    "        val_gt_binary_images_paths = val_image_paths_info['gt_binary_path_info']\n",
    "        val_gt_instance_images_paths = val_image_paths_info['gt_instance_path_info']\n",
    "\n",
    "        # split validation images according step size\n",
    "        val_split_result = _split_writing_tfrecords_task(\n",
    "            val_gt_images_paths, val_gt_binary_images_paths, val_gt_instance_images_paths, _flags='val')\n",
    "        val_example_gt_paths = val_split_result['gt_paths']\n",
    "        val_example_gt_binary_paths = val_split_result['gt_binary_paths']\n",
    "        val_example_gt_instance_paths = val_split_result['gt_instance_paths']\n",
    "        val_example_tfrecords_paths = val_split_result['tfrecords_paths']\n",
    "\n",
    "        for index, example_gt_paths in enumerate(val_example_gt_paths):\n",
    "            tf_io_pipline_tools.write_example_tfrecords(\n",
    "                example_gt_paths,\n",
    "                val_example_gt_binary_paths[index],\n",
    "                val_example_gt_instance_paths[index],\n",
    "                val_example_tfrecords_paths[index]\n",
    "            )\n",
    "\n",
    "        log.info('Generating validation example tfrecords complete')\n",
    "\n",
    "        # generate test example tfrecords\n",
    "        log.info('Start generating testing example tfrecords')\n",
    "\n",
    "        # collecting test images paths info\n",
    "        test_image_paths_info = _read_training_example_index_file(self._test_example_index_file_path)\n",
    "        test_gt_images_paths = test_image_paths_info['gt_path_info']\n",
    "        test_gt_binary_images_paths = test_image_paths_info['gt_binary_path_info']\n",
    "        test_gt_instance_images_paths = test_image_paths_info['gt_instance_path_info']\n",
    "\n",
    "        # split validating images according step size\n",
    "        test_split_result = _split_writing_tfrecords_task(\n",
    "            test_gt_images_paths, test_gt_binary_images_paths, test_gt_instance_images_paths, _flags='test')\n",
    "        test_example_gt_paths = test_split_result['gt_paths']\n",
    "        test_example_gt_binary_paths = test_split_result['gt_binary_paths']\n",
    "        test_example_gt_instance_paths = test_split_result['gt_instance_paths']\n",
    "        test_example_tfrecords_paths = test_split_result['tfrecords_paths']\n",
    "\n",
    "        for index, example_gt_paths in enumerate(test_example_gt_paths):\n",
    "            tf_io_pipline_tools.write_example_tfrecords(\n",
    "                example_gt_paths,\n",
    "                test_example_gt_binary_paths[index],\n",
    "                test_example_gt_instance_paths[index],\n",
    "                test_example_tfrecords_paths[index]\n",
    "            )\n",
    "\n",
    "        log.info('Generating testing example tfrecords complete')\n",
    "\n",
    "        return\n",
    "\n",
    "    def _is_source_data_complete(self):\n",
    "        \"\"\"\n",
    "        Check if source data complete\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return \\\n",
    "            ops.exists(self._gt_binary_image_dir) and \\\n",
    "            ops.exists(self._gt_instance_image_dir) and \\\n",
    "            ops.exists(self._gt_image_dir)\n",
    "\n",
    "    def _is_training_sample_index_file_complete(self):\n",
    "        \"\"\"\n",
    "        Check if the training sample index file is complete\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return \\\n",
    "            ops.exists(self._train_example_index_file_path) and \\\n",
    "            ops.exists(self._test_example_index_file_path) and \\\n",
    "            ops.exists(self._val_example_index_file_path)\n",
    "\n",
    "    def _generate_training_example_index_file(self):\n",
    "        \"\"\"\n",
    "        Generate training example index file, split source file into 0.85, 0.1, 0.05 for training,\n",
    "        testing and validation. Each image folder are processed separately\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        def _gather_example_info():\n",
    "            \"\"\"\n",
    "\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            _info = []\n",
    "\n",
    "            for _gt_image_path in glob.glob('{:s}/*.png'.format(self._gt_image_dir)):\n",
    "                _gt_binary_image_name = ops.split(_gt_image_path)[1]\n",
    "                _gt_binary_image_path = ops.join(self._gt_binary_image_dir, _gt_binary_image_name)\n",
    "                _gt_instance_image_name = ops.split(_gt_image_path)[1]\n",
    "                _gt_instance_image_path = ops.join(self._gt_instance_image_dir, _gt_instance_image_name)\n",
    "\n",
    "                assert ops.exists(_gt_binary_image_path), '{:s} not exist'.format(_gt_binary_image_path)\n",
    "                assert ops.exists(_gt_instance_image_path), '{:s} not exist'.format(_gt_instance_image_path)\n",
    "\n",
    "                _info.append('{:s} {:s} {:s}\\n'.format(\n",
    "                    _gt_image_path,\n",
    "                    _gt_binary_image_path,\n",
    "                    _gt_instance_image_path)\n",
    "                )\n",
    "\n",
    "            return _info\n",
    "\n",
    "        def _split_training_examples(_example_info):\n",
    "            random.shuffle(_example_info)\n",
    "\n",
    "            _example_nums = len(_example_info)\n",
    "\n",
    "            _train_example_info = _example_info[:int(_example_nums * 0.85)]\n",
    "            _val_example_info = _example_info[int(_example_nums * 0.85):int(_example_nums * 0.9)]\n",
    "            _test_example_info = _example_info[int(_example_nums * 0.9):]\n",
    "\n",
    "            return _train_example_info, _test_example_info, _val_example_info\n",
    "\n",
    "        train_example_info, test_example_info, val_example_info = _split_training_examples(_gather_example_info())\n",
    "\n",
    "        random.shuffle(train_example_info)\n",
    "        random.shuffle(test_example_info)\n",
    "        random.shuffle(val_example_info)\n",
    "\n",
    "        with open(ops.join(self._dataset_dir, 'train.txt'), 'w') as file:\n",
    "            file.write(''.join(train_example_info))\n",
    "\n",
    "        with open(ops.join(self._dataset_dir, 'test.txt'), 'w') as file:\n",
    "            file.write(''.join(test_example_info))\n",
    "\n",
    "        with open(ops.join(self._dataset_dir, 'val.txt'), 'w') as file:\n",
    "            file.write(''.join(val_example_info))\n",
    "\n",
    "        log.info('Generating training example index file complete')\n",
    "\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneNetDataFeeder(object):\n",
    "    \"\"\"\n",
    "    Read training examples from tfrecords for nsfw model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_dir, flags='train'):\n",
    "        \"\"\"\n",
    "\n",
    "        :param dataset_dir:\n",
    "        :param flags:\n",
    "        \"\"\"\n",
    "        self._dataset_dir = dataset_dir\n",
    "\n",
    "        self._tfrecords_dir = ops.join(dataset_dir, 'tfrecords')\n",
    "        if not ops.exists(self._tfrecords_dir):\n",
    "            raise ValueError('{:s} not exist, please check again'.format(self._tfrecords_dir))\n",
    "\n",
    "        self._dataset_flags = flags.lower()\n",
    "        if self._dataset_flags not in ['train', 'test', 'val']:\n",
    "            raise ValueError('flags of the data feeder should be \\'train\\', \\'test\\', \\'val\\'')\n",
    "\n",
    "    def inputs(self, batch_size, num_epochs):\n",
    "        \"\"\"\n",
    "        dataset feed pipline input\n",
    "        :param batch_size:\n",
    "        :param num_epochs:\n",
    "        :return: A tuple (images, labels), where:\n",
    "                    * images is a float tensor with shape [batch_size, H, W, C]\n",
    "                      in the range [-0.5, 0.5].\n",
    "                    * labels is an int32 tensor with shape [batch_size] with the true label,\n",
    "                      a number in the range [0, CLASS_NUMS).\n",
    "        \"\"\"\n",
    "        if not num_epochs:\n",
    "            num_epochs = None\n",
    "\n",
    "        tfrecords_file_paths = glob.glob('{:s}/{:s}*.tfrecords'.format(\n",
    "            self._tfrecords_dir, self._dataset_flags)\n",
    "        )\n",
    "        random.shuffle(tfrecords_file_paths)\n",
    "\n",
    "        with tf.name_scope('input_tensor'):\n",
    "\n",
    "            # TFRecordDataset opens a binary file and reads one record at a time.\n",
    "            # `tfrecords_file_paths` could also be a list of filenames, which will be read in order.\n",
    "            dataset = tf.data.TFRecordDataset(tfrecords_file_paths)\n",
    "\n",
    "            # The map transformation takes a function and applies it to every element\n",
    "            # of the dataset.\n",
    "            dataset = dataset.map(map_func=tf_io_pipline_tools.decode,\n",
    "                                  num_parallel_calls=CFG.TRAIN.CPU_MULTI_PROCESS_NUMS)\n",
    "            if self._dataset_flags != 'test':\n",
    "                dataset = dataset.map(map_func=tf_io_pipline_tools.augment_for_train,\n",
    "                                      num_parallel_calls=CFG.TRAIN.CPU_MULTI_PROCESS_NUMS)\n",
    "            else:\n",
    "                dataset = dataset.map(map_func=tf_io_pipline_tools.augment_for_test,\n",
    "                                      num_parallel_calls=CFG.TRAIN.CPU_MULTI_PROCESS_NUMS)\n",
    "            dataset = dataset.map(map_func=tf_io_pipline_tools.normalize,\n",
    "                                  num_parallel_calls=CFG.TRAIN.CPU_MULTI_PROCESS_NUMS)\n",
    "\n",
    "            # The shuffle transformation uses a finite-sized buffer to shuffle elements\n",
    "            # in memory. The parameter is the number of elements in the buffer. For\n",
    "            # completely uniform shuffling, set the parameter to be the same as the\n",
    "            # number of elements in the dataset.\n",
    "            if self._dataset_flags != 'test':\n",
    "                dataset = dataset.shuffle(buffer_size=1000)\n",
    "                # repeat num epochs\n",
    "                dataset = dataset.repeat()\n",
    "\n",
    "            dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "            iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "        return iterator.get_next(name='{:s}_IteratorGetNext'.format(self._dataset_flags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset_dir DATASET_DIR]\n",
      "                             [--tfrecords_dir TFRECORDS_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\yaboy\\AppData\\Roaming\\jupyter\\runtime\\kernel-39d4f4a2-e6dd-4964-8e53-62cc3fc586df.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0630 15:08:11.175494 7564 warnings.py:99] D:\\programs\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %tb\n",
    "args = init_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
